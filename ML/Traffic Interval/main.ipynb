{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data and sort them by time\n",
    "df = pd.read_csv(\"./Data/traffic.csv\")\n",
    "df['DateTime'] = pd.to_datetime(df['DateTime'])\n",
    "df.sort_values(by=['DateTime'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode Junction as a numerical feature\n",
    "junction_encoder = LabelEncoder()\n",
    "df['Junction_encoded'] = junction_encoder.fit_transform(df['Junction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract time-based features for better pattern recognization\n",
    "df['Hour'] = df['DateTime'].dt.hour\n",
    "df['DayOfWeek'] = df['DateTime'].dt.dayofweek\n",
    "df['Month'] = df['DateTime'].dt.month\n",
    "df['IsWeekend'] = (df['DayOfWeek'] >= 5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize data\n",
    "\n",
    "scaler_dict = {}\n",
    "numerical_features = ['Vehicles', 'Hour', 'DayOfWeek', 'Month', 'Junction_encoded']\n",
    "for feature in numerical_features:\n",
    "    scaler = MinMaxScaler()\n",
    "    df[f'{feature}_normalized'] = scaler.fit_transform(df[[feature]])\n",
    "    scaler_dict[feature] = scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each junction (can possibly be 1), create a RNN/LSTM-like training sequence\n",
    "\n",
    "def create_sequences_by_junction(data, seq_length):\n",
    "    feature_cols = [col for col in data.columns if '_normalized' in col]\n",
    "    sequences = []\n",
    "    labels = []\n",
    "    \n",
    "    # Group by junction and create sequences for each junction\n",
    "    for junction in data['Junction'].unique():\n",
    "        junction_data = data[data['Junction'] == junction].copy()\n",
    "        \n",
    "        for i in range(len(junction_data) - seq_length):\n",
    "            seq = junction_data[feature_cols].iloc[i:i+seq_length].values\n",
    "            label = junction_data['Vehicles_normalized'].iloc[i+seq_length]\n",
    "            sequences.append(seq)\n",
    "            labels.append(label)\n",
    "    \n",
    "    return np.array(sequences), np.array(labels)\n",
    "\n",
    "seq_length = 10  # Lookback window, can be modified\n",
    "\n",
    "# Create data\n",
    "#X, y = create_sequences_by_junction(df, seq_length)\n",
    "#X = torch.tensor(X, dtype=torch.float32)\n",
    "#y = torch.tensor(y, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train test split\n",
    "\n",
    "train_size = int(0.8 * len(X))\n",
    "X_train, X_test = X[:train_size], X[train_size:]\n",
    "y_train, y_test = y[:train_size], y[train_size:]\n",
    "train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(TensorDataset(X_test, y_test), batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic LSTM model\n",
    "class JunctionTrafficLSTM(nn.Module):\n",
    "    def __init__(self, input_size=5, hidden_size=32, num_layers=5, output_size=1):\n",
    "        super(JunctionTrafficLSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.fc1 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc4 = nn.Linear(hidden_size, hidden_size)        \n",
    "        self.fc5 = nn.Linear(hidden_size, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        out = self.dropout(lstm_out[:, -1, :])\n",
    "        out = self.relu(self.fc1(out))\n",
    "        out = self.relu(self.fc2(out))\n",
    "        out = self.relu(self.fc3(out))\n",
    "        out = self.relu(self.fc4(out))\n",
    "        return self.fc5(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model training set up\n",
    "\n",
    "# Use GPU for better performance if applicable\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = JunctionTrafficLSTM().to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, num_epochs=50, patience=10):\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs.squeeze(), batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch_X, batch_y in val_loader:\n",
    "                batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "                outputs = model(batch_X)\n",
    "                val_loss += criterion(outputs.squeeze(), batch_y).item()\n",
    "        train_loss /= len(train_loader)\n",
    "        val_loss /= len(val_loader)\n",
    "        \n",
    "        # Learning rate scheduling\n",
    "        scheduler.step(val_loss)\n",
    "        print(f\"Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "        \n",
    "        # Early stopping check\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(\"Early stopping triggered\")\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(model, test_loader, threshold):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_X, batch_y in test_loader:\n",
    "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "            outputs = model(batch_X)\n",
    "            difference = torch.abs(outputs.squeeze() - batch_y)\n",
    "            correct += (difference <= threshold).sum().item()\n",
    "            total += batch_y.size(0)\n",
    "    \n",
    "    accuracy_percentage = (correct / total) * 100\n",
    "    print(f\"Test Accuracy: {accuracy_percentage:.2f}%\")\n",
    "    return accuracy_percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_traffic(model, input_seq, junction, scalers):\n",
    "    model.eval()\n",
    "    input_seq = torch.tensor(input_seq, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        prediction = model(input_seq).cpu().numpy()\n",
    "    \n",
    "    # Inverse transform the prediction\n",
    "    return scalers['Vehicles'].inverse_transform(prediction.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_size = int(0.1 * len(X_train))\n",
    "X_val = X_train[-val_size:]\n",
    "y_val = y_train[-val_size:]\n",
    "X_train = X_train[:-val_size]\n",
    "y_train = y_train[:-val_size]\n",
    "\n",
    "val_loader = DataLoader(TensorDataset(X_val, y_val), batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 0.0035, Val Loss: 0.0027\n",
      "Epoch 2, Train Loss: 0.0010, Val Loss: 0.0017\n",
      "Epoch 3, Train Loss: 0.0007, Val Loss: 0.0011\n",
      "Epoch 4, Train Loss: 0.0006, Val Loss: 0.0010\n",
      "Epoch 5, Train Loss: 0.0005, Val Loss: 0.0011\n",
      "Epoch 6, Train Loss: 0.0005, Val Loss: 0.0010\n",
      "Epoch 7, Train Loss: 0.0005, Val Loss: 0.0010\n",
      "Epoch 8, Train Loss: 0.0005, Val Loss: 0.0010\n",
      "Epoch 9, Train Loss: 0.0005, Val Loss: 0.0010\n",
      "Epoch 10, Train Loss: 0.0004, Val Loss: 0.0009\n",
      "Epoch 11, Train Loss: 0.0004, Val Loss: 0.0012\n",
      "Epoch 12, Train Loss: 0.0004, Val Loss: 0.0010\n",
      "Epoch 13, Train Loss: 0.0004, Val Loss: 0.0010\n",
      "Epoch 14, Train Loss: 0.0004, Val Loss: 0.0011\n",
      "Epoch 15, Train Loss: 0.0004, Val Loss: 0.0010\n",
      "Epoch 16, Train Loss: 0.0004, Val Loss: 0.0009\n",
      "Epoch 17, Train Loss: 0.0004, Val Loss: 0.0009\n",
      "Epoch 18, Train Loss: 0.0004, Val Loss: 0.0009\n",
      "Epoch 19, Train Loss: 0.0004, Val Loss: 0.0009\n",
      "Epoch 20, Train Loss: 0.0004, Val Loss: 0.0009\n",
      "Epoch 21, Train Loss: 0.0004, Val Loss: 0.0010\n",
      "Epoch 22, Train Loss: 0.0004, Val Loss: 0.0010\n",
      "Epoch 23, Train Loss: 0.0004, Val Loss: 0.0012\n",
      "Epoch 24, Train Loss: 0.0004, Val Loss: 0.0011\n",
      "Epoch 25, Train Loss: 0.0004, Val Loss: 0.0010\n",
      "Epoch 26, Train Loss: 0.0004, Val Loss: 0.0010\n",
      "Epoch 27, Train Loss: 0.0003, Val Loss: 0.0010\n",
      "Epoch 28, Train Loss: 0.0003, Val Loss: 0.0010\n",
      "Epoch 29, Train Loss: 0.0003, Val Loss: 0.0010\n",
      "Epoch 30, Train Loss: 0.0003, Val Loss: 0.0010\n",
      "Early stopping triggered\n"
     ]
    }
   ],
   "source": [
    "patience = 10\n",
    "train_model(model, train_loader=DataLoader(TensorDataset(X_train, y_train), \n",
    "            batch_size=32, shuffle=True),\n",
    "            val_loader=val_loader,patience=patience)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Basic Accuracy Evaluation:\n",
      "Threshold is 0.02, which corresponds to 3 car(s) uncertainty\n",
      "Predicted values within this uncertainty are considered correct in the calculation of accuracy\n",
      "Test Accuracy: 68.30%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "68.30282861896838"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "threshold = 0.02\n",
    "uncertainty = (scaler_dict['Vehicles'].inverse_transform([[threshold]]) - \n",
    "                scaler_dict['Vehicles'].inverse_transform([[0]]))[0][0]\n",
    "print(f\"\\nBasic Accuracy Evaluation:\")\n",
    "print(f\"Threshold is {threshold}, which corresponds to {int(uncertainty)} car(s) uncertainty\")\n",
    "print(\"Predicted values within this uncertainty are considered correct in the calculation of accuracy\")\n",
    "accuracy(model, test_loader, threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def predict_future_traffic(model, latest_time, num_hours, scalers, seq_length=10):\n",
    "    \"\"\"\n",
    "    Predict traffic for the future time period (in hours) starting from the latest_time.\n",
    "    The model is used to predict at hourly intervals.\n",
    "\n",
    "    :param model: Trained LSTM model\n",
    "    :param latest_time: The latest time in the dataset to start predictions (in datetime format)\n",
    "    :param num_hours: Number of hours to predict into the future\n",
    "    :param scalers: Dictionary of scalers for normalizing and inverse transforming data\n",
    "    :param seq_length: Length of input sequence for the model (default is 10)\n",
    "    :return: List of predicted traffic values (in number of vehicles) for each hour in the future\n",
    "    \"\"\"\n",
    "    \n",
    "    model.eval()\n",
    "    closest_time_idx = (df['DateTime'] - latest_time).abs().argmin()\n",
    "\n",
    "    print((df['DateTime'] - latest_time).abs().min())\n",
    "\n",
    "    latest_data = df.iloc[closest_time_idx]\n",
    "\n",
    "    input_seq = []\n",
    "    for i in range(seq_length):\n",
    "        # Ensure the values are converted to a numeric type (np.float32)\n",
    "        values = latest_data[['Vehicles_normalized', \n",
    "                              'Hour_normalized', 'DayOfWeek_normalized', \n",
    "                              'Month_normalized', 'Junction_encoded_normalized']].values.astype(np.float32)\n",
    "        input_seq.append(values)\n",
    "        \n",
    "        if closest_time_idx + i + 1 < len(df):\n",
    "            latest_data = df.iloc[closest_time_idx + i + 1]\n",
    "        else:\n",
    "            break\n",
    "            \n",
    "    input_seq = np.array(input_seq, dtype=np.float32)\n",
    "    \n",
    "    predictions = []\n",
    "\n",
    "    for _ in range(num_hours):\n",
    "        input_seq_tensor = torch.tensor(input_seq, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            prediction = model(input_seq_tensor).cpu().numpy()\n",
    "        \n",
    "        # Inverse transform the prediction\n",
    "        predicted_vehicles = scalers['Vehicles'].inverse_transform(prediction.reshape(-1, 1))\n",
    "        predictions.append(predicted_vehicles[0][0])  # Append only the predicted value (first value of the prediction)\n",
    "        \n",
    "        # Update the input sequence for the next prediction\n",
    "        input_seq = np.roll(input_seq, shift=-1, axis=0)\n",
    "        \n",
    "        # Create a new input array with predicted values, keeping the same feature count\n",
    "        new_input = np.array([[ \n",
    "            predicted_vehicles[0][0],  # Predicted vehicle count\n",
    "            input_seq[-1][1],           # Hour\n",
    "            input_seq[-1][2],           # Day of the week\n",
    "            input_seq[-1][3],           # Month\n",
    "            input_seq[-1][4]            # Junction encoding\n",
    "        ]], dtype=np.float32)\n",
    "        \n",
    "        # Add the new prediction into the input sequence\n",
    "        input_seq[-1] = new_input[0]  # Replace the last entry with new input  \n",
    "    \n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "301 days 15:00:00\n",
      "        Predicted Time  Predicted Traffic (Vehicles)\n",
      "0  2015-01-03 09:00:00                     11.357082\n",
      "1  2015-01-03 10:00:00                     92.941544\n",
      "2  2015-01-03 11:00:00                    108.838860\n",
      "3  2015-01-03 12:00:00                    149.109604\n",
      "4  2015-01-03 13:00:00                    167.725433\n",
      "5  2015-01-03 14:00:00                    167.499908\n",
      "6  2015-01-03 15:00:00                    166.327026\n",
      "7  2015-01-03 16:00:00                    165.413773\n",
      "8  2015-01-03 17:00:00                    163.312180\n",
      "9  2015-01-03 18:00:00                    160.947250\n",
      "10 2015-01-03 19:00:00                    158.240295\n",
      "11 2015-01-03 20:00:00                    156.708755\n",
      "12 2015-01-03 21:00:00                    156.672165\n",
      "13 2015-01-03 22:00:00                    156.668472\n",
      "14 2015-01-03 23:00:00                    156.675430\n",
      "15 2015-01-04 00:00:00                    156.677017\n",
      "16 2015-01-04 01:00:00                    156.674347\n",
      "17 2015-01-04 02:00:00                    156.685455\n",
      "18 2015-01-04 03:00:00                    156.687729\n",
      "19 2015-01-04 04:00:00                    156.686844\n",
      "20 2015-01-04 05:00:00                    156.702682\n",
      "21 2015-01-04 06:00:00                    156.676636\n",
      "22 2015-01-04 07:00:00                    156.667969\n",
      "23 2015-01-04 08:00:00                    156.664001\n"
     ]
    }
   ],
   "source": [
    "latest_time = pd.to_datetime(\"2015-11-03 09:00:00\")  # Example, replace with actual latest time\n",
    "\n",
    "num_hours = 24  # Predict the next 24 hours\n",
    "\n",
    "model.load_state_dict(torch.load('best_model.pth'))\n",
    "\n",
    "predictions = predict_future_traffic(model, latest_time, num_hours, scaler_dict)\n",
    "\n",
    "# Create a DataFrame to store the predictions\n",
    "predicted_times = [latest_time + timedelta(hours=i) for i in range(num_hours)]\n",
    "predicted_traffic_df = pd.DataFrame({\n",
    "    'Predicted Time': predicted_times,\n",
    "    'Predicted Traffic (Vehicles)': predictions\n",
    "})\n",
    "\n",
    "# Print the DataFrame\n",
    "print(predicted_traffic_df)\n",
    "\n",
    "# Optionally, save to a CSV file\n",
    "predicted_traffic_df.to_csv('predicted_traffic.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
